# 神经网络

## 组成

* 输入层
* 中间层(隐藏层)
* 输出层

> 神经网络层数
>
> * 根据实际上拥有权重的层数(输入层、隐藏层、输出层的总数减去1)来表示网络的层数
> * 输入层为第0层
> * 中间层为第1层
> * 输出层为第2层

## 激活函数

$$
h(x)=
\begin {cases}
0\ (x\ \leq\ 0)\\
1\ (x\ >\ 0)
\end {cases}
$$

* 定义

  ​	将输入信号的总和转换为输出信号

* 种类

  * 阶跃函数

    ​	以阈值为界，一旦超过阈值就会切换输出
    $$
    y=\begin{cases}0 \quad (w_1x_1 + w_2x_2\leq\theta) \\1 \quad (w_1x_1 + w_2x_2\ >\theta)\end{cases}
    $$

  * sigmoid 函数

    $$
    h(x) = 1/1+exp(-x)
    $$
    
  * ReLU 函数
    $$
    h(x) = 
    \begin{cases}
    x\ (x\ >\ 0) \\
    0\ (x\ \leq\ 0)
    \end{cases}
    $$
    
    > 函数
    >
    > * 作用
    >
    >   ​	给定某个输入后，会返回某个输出的转换器
    >
    > * 种类
    >
    >   * 非线性函数
    >
    >     ​	输出值是一条连续变化的函数
    >
    >   * 线性函数
    >
    >     ​	输出值是常数倍的函数(数学公式表示为$h(x) = cx$
    >
    > **<font color=red>神经网络的激活函数必须使用非线性函数</font>**
    >
    > p.s. 如果使用线性函数的话，加深神经网络的层数就失去其意义了
    >
    > 如果使用线性函数，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”
    >
    > 所以为了发挥叠加层所带来的优势，激活函数<font color=red>必须使用**非线性函数**</font>
    >
    > 即感知机中神经元(节点)之间流动的是0或1的二元信号，而神经网络中流动的是连续的实数值信号
  
  > 对感知机公式的进一步优化
  > $$
  > y=\begin{cases}0 \quad (b + w_1x_1 + w_2x_2\leq\ 0) \\1 \quad (b + w_1z_1 + w_2x_2\ >\ 0)\end{cases}
  > $$
  > $a = b + w_1x_1 + w_2x_2$
  >
  > $y = h(a)$
  
* 共性

  * 当输入信号为重要信息时，输出较大的值；当输出信号为不重要的信息时，输出较小的值
  * 不管输入信号有多小，或者有多大，输出信号的值都在0到1之间

* 作用

  ​	决定如何来激活输入信号的总和

* 意义

  ​	链接感知机和神经网络的桥梁

  > * “朴素感知机”是指单层网络，指的是激活函数使用了阶跃函数的模型
  >
  > * “多层感知机”是指神经网络，即使用 sigmoid 函数等平滑的激活函数的多层网络

* 计算过程

  ​	信号的加权总和为节点 a ，然后节点 a 被激活函数 h() 转化成节点 y

> 跃阶函数、sigmoid 函数、RuLU 函数实现、形状与比较
>
> * 跃阶函数
>
>   简单实现：
>
>   ```python
>   def step_function(x): 
>     if x > 0:
>   		return 1 
>   	else:
>   		return 0
>   ```
>
>   Numpy 数组实现：
>
>   ```python
>   import numpy as np
>   
>   def step_function(x): 
>     y=x> 0
>     return y.astype(np.int) # astype() 方法转换 Numpy 数组的类型，并且可以通过参数指定期望的类型
>   ```
>
>   函数图像：
>
>   ```python
>   import numpy as np
>   import matplotlib.pylab as plt
>   
>   def step_function(x):
>     return np.array(x > 0, dtype=np.int)
>   
>   x = np.arange(-5.0, 5.0, 0.1)
>   y = step_function(x)
>   plt.plot(x, y)
>   plt.ylim(-0.1, 1.1) # 指定y轴的范围
>   plt.show()
>   ```
>   
> * sigmoid 函数
>
>   * 简单实现
>
>     ```python
>     import numpy as np
>     
>     def sigmoid(x):
>       return 1/(1+np.exp(-x))
>     ```
>   
>   * 函数图像
>   
>     ```python
>     import numpy as np
>     import matplotlib.pylab as plt
>               
>     def sigmoid(x):
>       return 1/(1+np.exp(-x))
>               
>     x = np.arange(-5.0, 5.0, 0.1)
>     y = sigmoid(x)
>     plt.plot(x, y)
>     plt.ylim(-0.1, 1.1) # 指定y轴的范围 
>     plt.show()
>     ```
>   
> * ReLU 函数
>
>   * 简单实现
>
>     ```python
>     import numpy as np
>     
>     def relu(x):
>       return np.maximum(0.x)
>     ```
>
>   * 函数图像
>
>     ```python
>     import numpy as np
>     import matplotlib.pylab as plt
>                         
>     def relu(x):
>       return np.maximum(0,x)
>                         
>     x= np.arange(-5.0,5.0,1.0)
>     y = relu(x)
>     plt.plot(x,y)
>     plt.ylim(-0.1,1.1)
>     plt.show()
>     ```
>

## 多维数组的运算

* 多维数组

  > ​		数字排成一列的集合、排成长方形的集合、排成三维状或者(更加一般化的)N维状的集合

  * 数组的位数可以通过 np.dim() 函数获得
  * 数组的形状可以通过实例变量 shape 获得，结果为<font color=red>元组(tuple)</font>
  * 二维数组也称为**矩阵**(matrix)
  * 数组的横向排列称为**行**(row),纵向排列称为**列**(column)

  ![截屏2023-05-28 15.08.32](/Users/wenruohan/Library/Application Support/typora-user-images/截屏2023-05-28 15.08.32.png)

* 矩阵乘法

  * 矩阵的乘积是通过左边矩阵的行(横向)和右边矩阵的列(纵向)以对应元素的方式相乘后再求和而得到的。并且，运算的结果保存为新的多维数组的元素
  * 乘积可以通过 NumPy 的 np.dot() 函数计算(乘积也称为点积)np.dot() 接收两个 NumPy 数组作为参 数，并返回数组的乘积。这里要注意的是, np.dot(A,B) 和 np.dot(B,A) 的值可能不一样。和一般的运算(+ 或 * 等)不同，矩阵的乘积运算中，操作数 (A,B) 的顺序不同，结果也会不同
  * 矩阵 A 的第 1 维的元素个数(列数) 必须和矩阵B的第0维的元素个数(行数)相等

 

*  神经网络的内积

  ```python
  X = np.array([1,2])
  W = np.array([[1,3,5],[2,4,6]])
  Y = np.dot(X,W)
  print(Y)
  ```

## 三层神经网络的实现

> 三层神经网络
>
> ​		输入层(第 0 层)有 2 个神经元，第 1 个隐藏层(第 1 层)有 3 个神经元， 第 2 个隐藏层(第 2 层)有 2 个神经元，输出层(第 3 层)有 2 个神经元

* 符号确认

> 神经网络的运算可以作为矩阵运算打包进行。因为神经网络各层的运算是通过矩阵的乘法运算打包进行的(从宏观视角来考虑)

![截屏2023-05-28 15.16.33](/Users/wenruohan/Library/Application Support/typora-user-images/截屏2023-05-28 15.16.33.png)

权重和隐藏层的神经元的右上角有一个“(1)”，它表示 权重和神经元的层号(即第1层的权重、第1层的神经元)。此外，权重的右 下角有两个数字，它们是后一层的神经元和前一层的神经元的索引号。权重右下角按照“后一层的索引号、前一层的索引号”的顺序排列。

* 各层间信号传递的实现

  ![截屏2023-05-28 15.17.16](/Users/wenruohan/Library/Application Support/typora-user-images/截屏2023-05-28 15.17.16.png)

​	偏置的右下角的索引号只有一个。这是因为前一层的偏置神经元(神经元“1”)只有一个

> 任何前一层的偏置神经元“1”都只有一个。偏置权重的数量取决于后一层的神经元的数量(不包括后一层的偏置神经元“1”)

* 数学表示

  $a_1^{(1)} = w_{1\ 1}^{(1)}x_1+w_{1\ 2}^{(1)}x_2+b_1^{(1)}$

* 矩阵运算简化

  $A^{(1)}=XW^{(1)}+B^{(1)}$

> $$
> A^{(1)} = 
> \left(
> \matrix{
>   a_1^{(1)} & a_2^{(1)} & a_3^{(1)}
> }
> \right)
> $$
>
> $$
> X =
> \left(
> \matrix{
> x_1 & x_2
> }
> \right)
> $$
>
> $$
> W =
> \left(
> \matrix{
> w_{1\ 1}^{(1)} & w_{2\ 1}^{(1)} & w_{3\ 1}^{(1)}\\
> w_{1\ 2}^{(1)} & w_{2\ 2}^{(1)} & w_{3\ 2}^{(1)}
> }
> \right)
> $$

* 代码实现

```python
# 输入层到第一层
X = np.array([1.0, 0.5])
W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) B1 = np.array([0.1, 0.2, 0.3])
print(W1.shape) # (2, 3)
print(X.shape) # (2,)
print(B1.shape) # (3,)
A1 = np.dot(X, W1) + B1

# 隐藏层
Z1 = sigmoid(A1)
print(A1) # [0.3, 0.7, 1.1]
print(Z1) # [0.57444252, 0.66818777, 0.75026011]
```

![截屏2023-05-28 15.43.55](/Users/wenruohan/Library/Application Support/typora-user-images/截屏2023-05-28 15.43.55.png)

```python
# 第一层到第二层
W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
B2 = np.array([0.1, 0.2])
print(Z1.shape) # (3,) print(W2.shape) # (3, 2) print(B2.shape) # (2,)
A2 = np.dot(Z1, W2) + B2 Z2 = sigmoid(A2)
```

![截屏2023-05-28 15.45.18](/Users/wenruohan/Library/Application Support/typora-user-images/截屏2023-05-28 15.45.18.png)

```python
# 第二层到输出层
def identity_function(x): return x
W3 = np.array([[0.1, 0.3], [0.2, 0.4]]) B3 = np.array([0.1, 0.2])
A3 = np.dot(Z2, W3) + B3
Y = identity_function(A3) # 或者Y = A3
'''
定义了 identity_function() 函数(也称为“恒等函数”)，并将 其作为输出层的激活函数。恒等函数会将输入按原样输出
输出层的激活函数用 σ() 表示，不同于隐 藏层的激活函数 h()(σ 读作 sigma)
'''
```

![截屏2023-05-28 15.48.49](/Users/wenruohan/Library/Application Support/typora-user-images/截屏2023-05-28 15.48.49.png)

> 输出层所用的激活函数，要根据求解问题的性质决定。一般地，回 归问题可以使用恒等函数，二元分类问题可以使用 sigmoid 函数， 多元分类问题可以使用 softmax 函数

* 代码小结

```python
def init_network(): 
  network = {}
	network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
	network['b1'] = np.array([0.1, 0.2, 0.3]) 
	network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) 
	network['b2'] = np.array([0.1, 0.2]) 
	network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
	network['b3'] = np.array([0.1, 0.2])
	return network

def forward(network, x):
	W1, W2, W3 = network['W1'], network['W2'], network['W3']
  b1, b2, b3 = network['b1'], network['b2'], network['b3']
	a1 = np.dot(x, W1) + b1
  z1 = sigmoid(a1)
	a2 = np.dot(z1, W2) + b2
  z2 = sigmoid(a2)
	a3 = np.dot(z2, W3) + b3
  y = identity_function(a3)
	return y
	
  network = init_network()
	x = np.array([1.0, 0.5])
	y = forward(network, x)
	print(y) # [ 0.31682708 0.69627909]
```

> init_network() 函数会进 行权重和偏置的初始化，并将它们保存在字典变量 network 中。这个字典变量 network 中 保 存 了 每 一 层 所 需 的 参 数 (权重和偏置) 。 forward() 函数中则封装了将输入信号转换为输出信号的处理过程。
>
> forward(前向)一词，它表示的是从输入到输出方向的传递处理

## 输出层设计

> 神经网络的应用场景
>
> * 分类问题
>
>   ​		数据属于哪一个类别的问题
>
> * 回归问题
>
>   ​		根据某个输入预测一个(连续的)数值的问题

* 需要根据情况改变输出层的激活函数

  * 回归问题——恒等函数

    ​		恒等函数会将输入按原样输出，对于输入的信息，不加以任何改动地直接输出
  
  
    * 分类问题——softmax 函数
  
      ​		$y_k = \frac{exp(a_k)}{\sum_{t=1}^{n}{exp(a_i)}}$
  
      > Softmax 函数
      >
      > * 作用
      >
      >   输出层的各个神经元都受到所有输入信号的影响
      >
      > * 特性
      >
      >   **输出总和为1**
      >
      >   softmax 函数的输出是0.0到1.0之间的实数。并且，softmax 函数的输出值的总和是1。*因为此特性，我们常常使用 softmax 函数表示**概率***
      >
      > * 注意事项
      >
      >   <font color=red>**溢出问题**</font>
      >
      >   softmax 函数的实现中要进行指数函数的运算，但是此时指数函数的值很容易变得非常大。比如，$e^{10}$的值 会超过20000，$e^{100}$会变成一个后面有 40 多个 0 的超大值，$e^{1000}$的结果会返回一个表示无穷大的 inf 。如果在这些超大值之间进行除法运算，结果会出现“不确定”的情况
      >
      >   [^溢出]:计 算 机 处 理“ 数 ”时 ，可以表示的数值范围是有限的，出现超大值无法表示的问题叫做**溢出**，因此，计 算 机 处 理“ 数 ”时 ，数值必须在4字节或8字节的有限数据宽度内 。 这意味着数存在有效位数。
      >
      >   处理：
      >
      >   在分子和分母上都乘上 C 这个任意的常数(因为同时对 分母和分子乘以相同的常数，所以计算结果不变)。然后，把这个 C 移动到 指数函数(exp)中，记为 log C。最后，把 log C 替换为另一个符号 C‘，在进行 softmax 的指数函数的运算时，加上(或者减去) 某个常数并不会改变运算的结果。这里的 C‘可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值
      >
      


* **输出层**神经元数量

  ​	**根据待解决的问题**来决定

  * 分类问题

    ​		一般设定为类别的数量
